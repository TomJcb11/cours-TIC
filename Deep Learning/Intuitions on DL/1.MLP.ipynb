{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 2: Multilayer Perceptron (MLP) and Backpropagation\n",
    "\n",
    "Now that the trainees understand the basics of a single perceptron, we'll introduce them to **multilayer neural networks** and dive deeper into backpropagation and optimization. We'll also start working with real-world datasets to see how these concepts come together in practice.\n",
    "\n",
    "#### 1. **Multilayer Perceptron (MLP)**\n",
    "A **Multilayer Perceptron (MLP)** consists of an input layer, one or more hidden layers, and an output layer. Each neuron in a layer is connected to every neuron in the next layer. This structure enables the model to learn more complex functions compared to a single-layer perceptron.\n",
    "\n",
    "##### How it works:\n",
    "- **Input Layer**: Takes the input features.\n",
    "- **Hidden Layers**: These layers apply transformations (via neurons and activation functions) to learn intermediate representations.\n",
    "- **Output Layer**: Produces the final prediction (e.g., for classification, the output could be probabilities for different classes).\n",
    "\n",
    "Each layer introduces more complexity, allowing the network to learn more about the patterns in the data.\n",
    "\n",
    "##### Formula for an MLP:\n",
    "Each hidden neuron computes the following:\n",
    "\n",
    "$$\n",
    "z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ l $ is the layer index.\n",
    "- $ W^{(l)} $ are the weights of layer $ l $.\n",
    "- $ a^{(l-1)} $ are the activations from the previous layer.\n",
    "- $ b^{(l)} $ are the biases of layer $ l $.\n",
    "\n",
    "The activations $ a^{(l)} $ are obtained by applying an activation function $ f $ to $ z^{(l)} $:\n",
    "\n",
    "$$\n",
    "a^{(l)} = f(z^{(l)})\n",
    "$$\n",
    "\n",
    "The final output layer uses either **sigmoid**, **softmax**, or **linear** activation depending on the problem (classification or regression).\n",
    "\n",
    "#### 2. **Backpropagation in MLP**\n",
    "In an MLP, backpropagation is used to update the weights for every layer in the network, not just the output layer.\n",
    "\n",
    "##### Steps of backpropagation:\n",
    "1. **Forward pass**: Calculate the output by passing inputs through all the layers.\n",
    "2. **Calculate loss**: Compare the network's output to the true output using a loss function.\n",
    "3. **Backpropagate the error**: Using the chain rule, compute the gradients of the loss function with respect to each weight in the network, working backward from the output layer to the input layer.\n",
    "4. **Update weights**: Update the weights using gradient descent:\n",
    "\n",
    "$$\n",
    "w_i^{(l)} \\gets w_i^{(l)} - \\eta \\frac{\\partial L}{\\partial w_i^{(l)}}\n",
    "$$\n",
    "\n",
    "Where $ \\eta $ is the learning rate and $ \\frac{\\partial L}{\\partial w_i^{(l)}} $ is the gradient of the loss with respect to the weight $ w_i^{(l)} $.\n",
    "\n",
    "\n",
    "#### 3. **Building an MLP in Code (from Scratch)**\n",
    "Let's now extend the perceptron code to build a simple MLP using NumPy.\n",
    "\n",
    "Let's break down this Multi-Layer Perceptron (MLP) implementation to make it easier to understand:\n",
    "\n",
    "### 1. **Imports**\n",
    "```python\n",
    "import numpy as np\n",
    "```\n",
    "- **`numpy`** is imported to handle array operations and matrix math efficiently, which is crucial for neural networks.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Activation Functions**\n",
    "```python\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "```\n",
    "- **Sigmoid Activation Function**: Converts any input into a value between `0` and `1`. This helps to introduce non-linearity into the neural network.\n",
    "  - Formula: $ \\sigma(x) = \\frac{1}{1 + e^{-x}} $\n",
    "- **Sigmoid Derivative**: This function is used to calculate how much to adjust the weights during backpropagation. The derivative of the sigmoid function is: \n",
    "  - $ \\sigma'(x) = \\sigma(x) \\cdot (1 - \\sigma(x)) $\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **MLP Class**\n",
    "#### 3.1. **Initialization (`__init__` Method)**\n",
    "```python\n",
    "class MLP:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.weights_input_hidden = np.random.rand(input_size, hidden_size)\n",
    "        self.weights_hidden_output = np.random.rand(hidden_size, output_size)\n",
    "        self.bias_hidden = np.random.rand(hidden_size)\n",
    "        self.bias_output = np.random.rand(output_size)\n",
    "```\n",
    "- **Weights**: Randomly initialized matrices for the connections between layers.\n",
    "  - **`weights_input_hidden`** connects the input layer to the hidden layer.\n",
    "  - **`weights_hidden_output`** connects the hidden layer to the output layer.\n",
    "- **Biases**: Randomly initialized values for each neuron in the hidden and output layers. The bias helps to shift the activation function.\n",
    "  - **`bias_hidden`** is for the hidden layer.\n",
    "  - **`bias_output`** is for the output layer.\n",
    "\n",
    "For example, if the network has:\n",
    "- **2 input neurons**, \n",
    "- **2 hidden neurons**, and \n",
    "- **1 output neuron**, \n",
    "\n",
    "then:\n",
    "- **`weights_input_hidden`** will be a 2x2 matrix.\n",
    "- **`weights_hidden_output`** will be a 2x1 matrix.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3.2. **Forward Pass (`forward` Method)**\n",
    "```python\n",
    "def forward(self, inputs):\n",
    "    self.hidden_layer_input = np.dot(inputs, self.weights_input_hidden) + self.bias_hidden\n",
    "    self.hidden_layer_output = sigmoid(self.hidden_layer_input)\n",
    "\n",
    "    self.output_layer_input = np.dot(self.hidden_layer_output, self.weights_hidden_output) + self.bias_output\n",
    "    self.output = sigmoid(self.output_layer_input)\n",
    "    \n",
    "    return self.output\n",
    "```\n",
    "- **Forward pass** calculates the output of the neural network by passing the input data through the layers.\n",
    "\n",
    "Steps:\n",
    "1. **Hidden Layer**:\n",
    "   - Compute the input to the hidden layer: \n",
    "     - $ hidden\\_layer\\_input = inputs \\cdot weights\\_input\\_hidden + bias\\_hidden $\n",
    "   - Apply the sigmoid activation function to get the output of the hidden layer: \n",
    "     - $ hidden\\_layer\\_output = sigmoid(hidden\\_layer\\_input) $\n",
    "2. **Output Layer**:\n",
    "   - Compute the input to the output layer:\n",
    "     - $ output\\_layer\\_input = hidden\\_layer\\_output \\cdot weights\\_hidden\\_output + bias\\_output $\n",
    "   - Apply the sigmoid activation function to get the network's final output: \n",
    "     - $ output = sigmoid(output\\_layer\\_input) $\n",
    "\n",
    "---\n",
    "\n",
    "#### 3.3. **Backward Pass (`backward` Method)**\n",
    "```python\n",
    "def backward(self, inputs, actual_output, predicted_output):\n",
    "    output_error = actual_output - predicted_output\n",
    "    output_delta = output_error * sigmoid_derivative(predicted_output)\n",
    "\n",
    "    hidden_error = output_delta.dot(self.weights_hidden_output.T)\n",
    "    hidden_delta = hidden_error * sigmoid_derivative(self.hidden_layer_output)\n",
    "\n",
    "    self.weights_hidden_output += self.hidden_layer_output.T.dot(output_delta)\n",
    "    self.bias_output += np.sum(output_delta, axis=0)\n",
    "\n",
    "    self.weights_input_hidden += inputs.T.dot(hidden_delta)\n",
    "    self.bias_hidden += np.sum(hidden_delta, axis=0)\n",
    "```\n",
    "- The **backward pass** adjusts the weights and biases using the error between the predicted and actual outputs. This process is called **backpropagation**.\n",
    "\n",
    "Steps:\n",
    "1. **Calculate Output Error**:\n",
    "   - **`output_error`**: Difference between the **actual output** and the **predicted output** (i.e., the error).\n",
    "   - **`output_delta`**: Error adjusted by the derivative of the sigmoid function. This helps determine how much each weight contributed to the error.\n",
    "   \n",
    "2. **Calculate Hidden Layer Error**:\n",
    "   - **`hidden_error`**: Propagate the output error back to the hidden layer using the weights connecting the hidden layer to the output.\n",
    "   - **`hidden_delta`**: Adjust the hidden error using the sigmoid derivative to calculate how much to adjust the weights between the input and hidden layer.\n",
    "\n",
    "3. **Update Weights and Biases**:\n",
    "   - Adjust the **weights and biases** by multiplying the deltas (errors) with the input or hidden outputs to update the connections:\n",
    "     - **`weights_hidden_output`**: Adjusted using the output delta and the hidden layer's output.\n",
    "     - **`weights_input_hidden`**: Adjusted using the hidden delta and the input values.\n",
    "     - **Biases** are updated by summing the deltas.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3.4. **Training (`train` Method)**\n",
    "```python\n",
    "def train(self, inputs, labels, epochs=10000):\n",
    "    for _ in range(epochs):\n",
    "        predicted_output = self.forward(inputs)\n",
    "        self.backward(inputs, labels, predicted_output)\n",
    "```\n",
    "- **Training** runs the forward pass and backward pass for a given number of **epochs** (iterations).\n",
    "- During each epoch, the neural network:\n",
    "  - **Forward pass**: Predicts the output using the current weights.\n",
    "  - **Backward pass**: Adjusts the weights and biases based on the error.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Training Data**\n",
    "```python\n",
    "inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "labels = np.array([[0], [1], [1], [0]])  # XOR gate\n",
    "```\n",
    "- **Inputs**: These are the inputs to the neural network. Each input pair represents the possible values of a binary XOR gate.\n",
    "  - XOR gate: \n",
    "    - $ 0 \\oplus 0 = 0 $\n",
    "    - $ 0 \\oplus 1 = 1 $\n",
    "    - $ 1 \\oplus 0 = 1 $\n",
    "    - $ 1 \\oplus 1 = 0 $\n",
    "- **Labels**: These are the expected outputs (or ground truth) for the XOR gate.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Creating and Training the MLP**\n",
    "```python\n",
    "mlp = MLP(input_size=2, hidden_size=2, output_size=1)\n",
    "mlp.train(inputs, labels)\n",
    "```\n",
    "- **Create the MLP** with:\n",
    "  - **2 input neurons** (because the XOR problem has 2 inputs),\n",
    "  - **2 hidden neurons**, and\n",
    "  - **1 output neuron** (since XOR has a binary output).\n",
    "- **Train the MLP** using the input data and labels.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Testing the MLP**\n",
    "```python\n",
    "print(mlp.forward(np.array([0, 0])))  # Expected output: ~0\n",
    "print(mlp.forward(np.array([1, 1])))  # Expected output: ~0\n",
    "```\n",
    "- After training, test the MLP on the inputs:\n",
    "  - For input `[0, 0]`, the expected output is approximately `0`.\n",
    "  - For input `[1, 1]`, the expected output is approximately `0`.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Concepts:\n",
    "- **Multi-Layer Perceptron (MLP)**: A feedforward neural network with one or more hidden layers. It can solve non-linear problems like XOR.\n",
    "- **Forward Pass**: The process of passing inputs through the network to get the output.\n",
    "- **Backward Pass**: The process of calculating errors and adjusting weights to minimize them.\n",
    "- **Training**: Repeatedly running forward and backward passes to make the model learn from data.\n",
    "\n",
    "\n",
    "\n",
    "The function $$ \\sigma(x) = \\frac{1}{1 + e^{-x}} $$ is known as the **sigmoid function**, commonly used in machine learning, especially in neural networks.\n",
    "---\n",
    "\n",
    "\n",
    "### Derivative Calculation Sigmoid\n",
    "\n",
    "To find the derivative $$ \\frac{d}{dx} \\sigma(x) $$, let us break down step by step.\n",
    "\n",
    "### Step 1: Identify the form of the function\n",
    "The function $$ \\sigma(x) = \\frac{1}{1 + e^{-x}} $$ is a composite function of the form:\n",
    "\n",
    "$$\n",
    "\\sigma(x) = f(g(x)) \\quad \\text{where} \\quad f(u) = \\frac{1}{u} \\quad \\text{and} \\quad g(x) = 1 + e^{-x}.\n",
    "$$\n",
    "\n",
    "We'll need to use the **chain rule** to differentiate this.\n",
    "\n",
    "### Step 2: Differentiate $$ f(u) = \\frac{1}{u} $$\n",
    "First, find the derivative of $$ f(u) = \\frac{1}{u} $$:\n",
    "\n",
    "$$\n",
    "\\frac{d}{du} \\left( \\frac{1}{u} \\right) = -\\frac{1}{u^2}.\n",
    "$$\n",
    "\n",
    "We'll apply this result after differentiating $$ g(x) $$.\n",
    "\n",
    "### Step 3: Differentiate $$ g(x) = 1 + e^{-x} $$\n",
    "Now, find the derivative of $$ g(x) = 1 + e^{-x} $$:\n",
    "\n",
    "$$\n",
    "\\frac{d}{dx} \\left( 1 + e^{-x} \\right) = 0 + (-e^{-x}) = -e^{-x}.\n",
    "$$\n",
    "\n",
    "### Step 4: Apply the chain rule\n",
    "The derivative of $$ \\sigma(x) $$ is:\n",
    "\n",
    "$$\n",
    "\\frac{d}{dx} \\sigma(x) = \\frac{d}{du} f(u) \\cdot \\frac{d}{dx} g(x) = -\\frac{1}{(1 + e^{-x})^2} \\cdot (-e^{-x}).\n",
    "$$\n",
    "\n",
    "### Step 5: Simplify the expression\n",
    "Now, simplify the result:\n",
    "\n",
    "$$\n",
    "\\frac{d}{dx} \\sigma(x) = \\frac{e^{-x}}{(1 + e^{-x})^2}.\n",
    "$$\n",
    "\n",
    "### Step 6: Express the result in terms of $$ \\sigma(x) $$\n",
    "Notice that $$ \\sigma(x) = \\frac{1}{1 + e^{-x}} $$. This allows us to express the derivative in terms of $$ \\sigma(x) $$ itself.\n",
    "\n",
    "- Since $$ \\sigma(x) = \\frac{1}{1 + e^{-x}} $$, we know that:\n",
    "  $$\n",
    "  1 - \\sigma(x) = \\frac{e^{-x}}{1 + e^{-x}}.\n",
    "  $$\n",
    "\n",
    "Thus, the derivative can be rewritten as:\n",
    "\n",
    "$$\n",
    "\\frac{d}{dx} \\sigma(x) = \\sigma(x) (1 - \\sigma(x)).\n",
    "$$\n",
    "\n",
    "### Final Result:\n",
    "The derivative of the sigmoid function is:\n",
    "\n",
    "$$\n",
    "\\frac{d}{dx} \\sigma(x) = \\sigma(x) (1 - \\sigma(x)).\n",
    "$$\n",
    "\n",
    "This is a key property of the sigmoid function and is commonly used in backpropagation in neural networks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.49618505]\n",
      "[0.50232594]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Activation functions and their derivatives\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "\n",
    "# MLP class\n",
    "class MLP:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.weights_input_hidden = np.random.rand(input_size, hidden_size)\n",
    "        self.weights_hidden_output = np.random.rand(hidden_size, output_size)\n",
    "        self.bias_hidden = np.random.rand(hidden_size)\n",
    "        self.bias_output = np.random.rand(output_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.hidden_layer_input = (\n",
    "            np.dot(inputs, self.weights_input_hidden) + self.bias_hidden\n",
    "        )\n",
    "        self.hidden_layer_output = sigmoid(self.hidden_layer_input)\n",
    "\n",
    "        self.output_layer_input = (\n",
    "            np.dot(self.hidden_layer_output, self.weights_hidden_output)\n",
    "            + self.bias_output\n",
    "        )\n",
    "        self.output = sigmoid(self.output_layer_input)\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, inputs, actual_output, predicted_output):\n",
    "        # Calculate error\n",
    "        output_error = actual_output - predicted_output\n",
    "        output_delta = output_error * sigmoid_derivative(predicted_output)\n",
    "\n",
    "        hidden_error = output_delta.dot(self.weights_hidden_output.T)\n",
    "        hidden_delta = hidden_error * sigmoid_derivative(self.hidden_layer_output)\n",
    "\n",
    "        # Update weights and biases\n",
    "        self.weights_hidden_output += self.hidden_layer_output.T.dot(output_delta)\n",
    "        self.bias_output += np.sum(output_delta, axis=0)\n",
    "\n",
    "        self.weights_input_hidden += inputs.T.dot(hidden_delta)\n",
    "        self.bias_hidden += np.sum(hidden_delta, axis=0)\n",
    "\n",
    "    def train(self, inputs, labels, epochs=100):\n",
    "        for _ in range(epochs):\n",
    "            predicted_output = self.forward(inputs)\n",
    "            self.backward(inputs, labels, predicted_output)\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        return self.forward(inputs)\n",
    "\n",
    "\n",
    "# Sample training data\n",
    "inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "labels = np.array([[0], [1], [1], [0]])  # XOR gate\n",
    "\n",
    "# Create MLP and train\n",
    "mlp = MLP(input_size=2, hidden_size=2, output_size=1)\n",
    "mlp.train(inputs, labels)\n",
    "\n",
    "# Test the MLP\n",
    "print(mlp.predict(np.array([0, 0])))  # Expected output: ~0\n",
    "print(mlp.predict(np.array([1, 0])))  # Expected output: ~0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this code:\n",
    "- The network has 2 input neurons, 2 hidden neurons, and 1 output neuron.\n",
    "- It uses the **sigmoid** activation function in both the hidden and output layers.\n",
    "- The **backward** function computes the gradients and updates the weights using backpropagation.\n",
    "\n",
    "#### 4. **Working with Real Data (MNIST Example)**\n",
    "Now that we’ve coded an MLP from scratch, let’s move to a more complex task using real-world data.\n",
    "\n",
    "We will use **TensorFlow/Keras** to simplify the implementation and train a network to recognize handwritten digits using the **MNIST** dataset.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
