{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Working with Real Data (MNIST Example)**\n",
    "Now that we’ve coded an MLP from scratch, let’s move to a more complex task using real-world data.\n",
    "\n",
    "We will use **TensorFlow/Keras** to simplify the implementation and train a network to recognize handwritten digits using the **MNIST** dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Code using Keras:\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "train_images = train_images.reshape((60000, 28 * 28)).astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28 * 28)).astype('float32') / 255\n",
    "\n",
    "# One-hot encode labels\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels)\n",
    "\n",
    "# Build the model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_images, train_labels, epochs=5, batch_size=128)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(f'Test accuracy: {test_acc}')\n",
    "```\n",
    "\n",
    "Here’s what this does:\n",
    "- Loads the **MNIST dataset** of handwritten digits.\n",
    "- Preprocesses the images (flattening them to vectors and normalizing the pixel values).\n",
    "- Defines a simple MLP with one hidden layer of 512 neurons using **ReLU** and a softmax output layer.\n",
    "- Trains the network on the training data.\n",
    "- Evaluates the accuracy on the test data.\n",
    "\n",
    "#### 5. **Explaining Key Concepts**\n",
    "\n",
    "- **Loss function**: We use **categorical cross-entropy** for multiclass classification problems.\n",
    "- **Optimization**: We use **Adam optimizer**, a variant of gradient descent, which automatically adjusts the learning rate during training.\n",
    "- **Evaluation**: After training, we test the model on the test data to see how well it generalizes to unseen examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Day 2 Conclusion\n",
    "Today, we covered:\n",
    "- Multilayer Perceptron (MLP) and how it extends the simple perceptron.\n",
    "- Backpropagation and gradient descent for training deep networks.\n",
    "- Hands-on coding, first by building an MLP from scratch and then using TensorFlow to work on real-world data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Imports**\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import mnist\n",
    "```\n",
    "\n",
    "- **`tensorflow`**: A popular open-source machine learning library developed by Google.\n",
    "- **`tensorflow.keras`**: TensorFlow's high-level neural networks API, which simplifies building and training models.\n",
    "- **`layers` and `models`**: Submodules used to construct neural network layers and models.\n",
    "- **`mnist`**: A dataset of handwritten digits (0-9), consisting of 60,000 training images and 10,000 testing images.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import mnist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## **2. Loading the MNIST Dataset**\n",
    "\n",
    "```python\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "```\n",
    "- The MNIST dataset is split into training and testing sets.\n",
    "  - **`train_images`**: 60,000 images used for training.\n",
    "  - **`train_labels`**: Corresponding labels for training images.\n",
    "  - **`test_images`**: 10,000 images used for testing.\n",
    "  - **`test_labels`**: Corresponding labels for test images.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Data Preprocessing**\n",
    "\n",
    "### **3.1 Reshaping and Normalizing Images**\n",
    "\n",
    "```python\n",
    "train_images = train_images.reshape((60000, 28 * 28)).astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28 * 28)).astype('float32') / 255\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "train_images = train_images.reshape((60000, 28 * 28)).astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28 * 28)).astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Reshaping**:\n",
    "  - Original shape: `(num_samples, 28, 28)`.\n",
    "  - Reshaped to: `(num_samples, 784)` to flatten the 28x28 images into 1D vectors.\n",
    "- **Normalization**:\n",
    "  - Pixel values are integers from 0 to 255.\n",
    "  - Dividing by 255 scales the values to the range [0, 1], which helps the neural network train more efficiently.\n",
    "\n",
    "### **3.2 One-Hot Encoding Labels**\n",
    "\n",
    "```python\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels)\n",
    "```\n",
    "\n",
    "- **One-Hot Encoding**:\n",
    "  - Converts integer labels (0-9) into binary class matrices.\n",
    "  - Example: Label `3` becomes `[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]`.\n",
    "- **Why One-Hot Encoding?**\n",
    "  - Necessary for multiclass classification with categorical cross-entropy loss.\n",
    "  - Allows the model to output probabilities for each class.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Do I have to do One-Hot Encoding?\n",
    "\n",
    "In deep learning, **loss functions** like **categorical cross-entropy** and **sparse categorical cross-entropy** are used for classification tasks. Both are used for multi-class classification problems, but the main difference between them lies in how the labels are encoded. Here's an explanation:\n",
    "\n",
    "### 1. **Categorical Cross-Entropy**\n",
    "- **Usage**: Used when your labels are **one-hot encoded**.\n",
    "  \n",
    "  One-hot encoding means that for a classification problem with $ N $ classes, each label is represented as a vector of length $ N $, where one element is 1 (indicating the correct class) and all other elements are 0.\n",
    "\n",
    "  Example:\n",
    "  - Suppose you have 3 classes (0, 1, 2).\n",
    "  - If the label is 2, it would be one-hot encoded as [0, 0, 1].\n",
    "\n",
    "- **Formula**:\n",
    "  $$\n",
    "  L = -\\sum_{i=1}^{N} y_i \\log(\\hat{y}_i)\n",
    "  $$\n",
    "  Where:\n",
    "  - $ y_i $ is the one-hot encoded true label.\n",
    "  - $ \\hat{y}_i $ is the predicted probability for class $ i $.\n",
    "  \n",
    "- **When to use**: When your output labels are already one-hot encoded. For instance, in Keras, if your labels are one-hot encoded, you would use `categorical_crossentropy` as your loss function.\n",
    "\n",
    "### 2. **Sparse Categorical Cross-Entropy**\n",
    "- **Usage**: Used when your labels are **integers** rather than one-hot encoded.\n",
    "\n",
    "  Instead of one-hot encoding, each label is represented by a single integer, where the value of the integer corresponds to the class index.\n",
    "\n",
    "  Example:\n",
    "  - Suppose you have 3 classes (0, 1, 2).\n",
    "  - If the label is 2, it is represented as the integer `2` (not as a one-hot vector like `[0, 0, 1]`).\n",
    "\n",
    "- **Formula**:\n",
    "  The formula is similar to categorical cross-entropy, but the difference is in the input labels. Instead of using one-hot encoded vectors, the labels are integer class indices.\n",
    "\n",
    "  $$\n",
    "  L = -\\log(\\hat{y}_c)\n",
    "  $$\n",
    "  Where:\n",
    "  - $ c $ is the correct class index (an integer).\n",
    "  - $ \\hat{y}_c $ is the predicted probability for the correct class $ c $.\n",
    "\n",
    "- **When to use**: When your labels are integers (i.e., class indices) rather than one-hot encoded vectors. This is common in Keras when the labels are not explicitly converted to one-hot vectors; you would use `sparse_categorical_crossentropy`.\n",
    "\n",
    "### Key Differences\n",
    "- **Label Format**:\n",
    "  - **Categorical Cross-Entropy**: Requires **one-hot encoded labels**.\n",
    "  - **Sparse Categorical Cross-Entropy**: Requires **integer labels**.\n",
    "\n",
    "- **Use Case**:\n",
    "  - If your dataset labels are one-hot encoded (each label is a vector like `[0, 0, 1]`), you should use **categorical cross-entropy**.\n",
    "  - If your dataset labels are integer-encoded (each label is a scalar like `2`), you should use **sparse categorical cross-entropy**.\n",
    "\n",
    "### 3. **Binary Cross-Entropy**\n",
    "- **Usage**: Used for **binary classification** problems where the output has two possible classes (0 or 1).\n",
    "\n",
    "- **Formula**:\n",
    "  $$\n",
    "  L = -[y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y})]\n",
    "  $$\n",
    "  Where:\n",
    "  - $ y $ is the true label (0 or 1).\n",
    "  - $ \\hat{y} $ is the predicted probability for class 1.\n",
    "\n",
    "- **When to use**: For binary classification tasks, where there are only two possible classes (e.g., spam or not spam, cat or dog).\n",
    "\n",
    "### 1. **Poisson Loss**\n",
    "**Use Case**:\n",
    "Used when modeling count data where the predictions are expected to follow a **Poisson distribution**.\n",
    "- **Description**:\n",
    "Poisson loss calculates the loss assuming that the data follows a Poisson distribution, which is commonly used for count-based data (like the number of events occurring within a fixed interval).\n",
    "\n",
    "### Summary:\n",
    "- **Categorical Cross-Entropy**: Use when labels are **one-hot encoded** for multi-class classification.\n",
    "- **Sparse Categorical Cross-Entropy**: Use when labels are **integers** (class indices) for multi-class classification.\n",
    "- **Binary Cross-Entropy**: Use for **binary classification** problems (two classes).\n",
    "\n",
    "These loss functions calculate how far the predicted probabilities are from the actual class labels, helping the model adjust weights to improve its predictions during training.\n",
    "Summary:\n",
    "Categorical Cross-Entropy: Used for multi-class classification with one-hot encoded labels.\n",
    "Sparse Categorical Cross-Entropy: Used for multi-class classification with integer encoded labels (more efficient than categorical cross-entropy).\n",
    "Binary Cross-Entropy: Used for binary classification (0/1 labels).\n",
    "Poisson Loss: Used for modeling count data where the data follows a Poisson distribution.\n",
    "Each loss function is designed for different types of classification problems and different label formats (one-hot vs. integer encoding), so you should choose based on your specific problem and how your data is structured.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# One-hot encode labels\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## **4. Building the Model**\n",
    "\n",
    "```python\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "```\n",
    "\n",
    "### **4.1 Model Architecture**\n",
    "\n",
    "- **`models.Sequential()`**:\n",
    "  - A linear stack of layers.\n",
    "  - Suitable for models where layers are added one after another.\n",
    "- **Alternatives to Sequential Model**:\n",
    "  - **Functional API**: Allows for complex architectures (e.g., models with multiple inputs/outputs, shared layers).\n",
    "    - Example:\n",
    "      ```python\n",
    "      inputs = tf.keras.Input(shape=(28 * 28,))\n",
    "      x = layers.Dense(512, activation='relu')(inputs)\n",
    "      outputs = layers.Dense(10, activation='softmax')(x)\n",
    "      model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "      ```\n",
    "  - **Subclassing API**: For custom models and layers by subclassing `tf.keras.Model` or `tf.keras.layers.Layer`.\n",
    "\n",
    "### **4.2 Layers**\n",
    "\n",
    "#### **4.2.1 Dense Layers**\n",
    "\n",
    "- **`layers.Dense`**:\n",
    "  - Also known as fully connected layers.\n",
    "  - Each neuron in the layer is connected to every neuron in the previous and next layers.\n",
    "- **Why Use Dense Layers?**\n",
    "  - Suitable for tasks where the relationship between input features and output is not spatially dependent.\n",
    "  - In this example, images are flattened, so spatial relationships are not preserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------\n",
    "#### **4.2.2 First Dense Layer**\n",
    "\n",
    "```python\n",
    "model.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "```\n",
    "\n",
    "- **Units**: 512 neurons.\n",
    "- **Activation Function**: `'relu'` (Rectified Linear Unit).\n",
    "  - **ReLU Activation**:\n",
    "    - Formula: $ f(x) = \\max(0, x) $\n",
    "    - Introduces non-linearity.\n",
    "    - Helps with the vanishing gradient problem.\n",
    "- **Input Shape**: `(28 * 28,)` (flattened 784-dimensional vector).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\didie\\anaconda3\\envs\\pythonAlgo\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build the model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### **4.2.3 Second Dense Layer**\n",
    "\n",
    "```python\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "```\n",
    "\n",
    "- **Units**: 10 neurons (one for each digit class 0-9).\n",
    "- **Activation Function**: `'softmax'`.\n",
    "  - **Softmax Activation**:\n",
    "    - Converts logits into probabilities that sum to 1.\n",
    "    - Suitable for multiclass classification.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.Dense(10, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----------------------------------------------------------------\n",
    "### **4.3 Activation Functions**\n",
    "\n",
    "#### **Common Activation Functions**:\n",
    "\n",
    "1. **Sigmoid**:\n",
    "   - Formula: $ f(x) = \\frac{1}{1 + e^{-x}} $\n",
    "   - Output range: (0, 1)\n",
    "   - Used in binary classification.\n",
    "   - **Not ideal** for hidden layers due to vanishing gradient.\n",
    "\n",
    "2. **Tanh**:\n",
    "   - Formula: $ f(x) = \\tanh(x) $\n",
    "   - Output range: (-1, 1)\n",
    "   - Centers data around zero.\n",
    "\n",
    "3. **ReLU**:\n",
    "   - Formula: $ f(x) = \\max(0, x) $\n",
    "   - Advantages:\n",
    "     - Computationally efficient.\n",
    "     - Mitigates vanishing gradient.\n",
    "   - Disadvantages:\n",
    "     - Dying ReLU problem (neurons stop activating).\n",
    "\n",
    "4. **Leaky ReLU**:\n",
    "   - Formula: $ f(x) = \\max(\\alpha x, x) $, where $ \\alpha $ is a small constant (e.g., 0.01).\n",
    "   - Allows a small gradient when the unit is not active.\n",
    "\n",
    "5. **ELU (Exponential Linear Unit)**:\n",
    "   - Combines benefits of ReLU and mitigates the dying ReLU problem.\n",
    "\n",
    "6. **Softmax**:\n",
    "   - Used in the output layer for multiclass classification.\n",
    "   - Converts outputs to probability distributions.\n",
    "\n",
    "**Choosing Activation Functions**:\n",
    "\n",
    "- **Hidden Layers**:\n",
    "  - Typically use ReLU, Leaky ReLU, or ELU.\n",
    "  - ReLU is a common default choice due to its simplicity and effectiveness.\n",
    "\n",
    "- **Output Layer**:\n",
    "  - **Binary Classification**: Use `'sigmoid'` with `'binary_crossentropy'` loss.\n",
    "  - **Multiclass Classification**: Use `'softmax'` with `'categorical_crossentropy'` loss.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Compiling the Model**\n",
    "\n",
    "```python\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "### **5.1 Optimizer**\n",
    "\n",
    "- **`optimizer='adam'`**:\n",
    "  - **Adam (Adaptive Moment Estimation)**:\n",
    "    - Combines RMSprop and momentum. Check bonus info below\n",
    "    - Adjusts learning rate adaptively for each parameter.\n",
    "    - Effective and widely used default optimizer.\n",
    "- **Alternative Optimizers**:\n",
    "\n",
    "1. **SGD (Stochastic Gradient Descent)**:\n",
    "   - Basic optimizer with a fixed learning rate.\n",
    "   - Can use momentum to improve convergence.\n",
    "   - Example:\n",
    "     ```python\n",
    "     optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
    "     ```\n",
    "\n",
    "2. **RMSprop**:\n",
    "   - Maintains per-parameter learning rates, adjusting them based on the average of recent magnitudes of the gradients.\n",
    "\n",
    "3. **Adagrad**:\n",
    "   - Adapts the learning rate for each parameter based on the historical gradients.\n",
    "\n",
    "4. **Adadelta**:\n",
    "   - Extension of Adagrad, reduces its aggressive, monotonically decreasing learning rate.\n",
    "\n",
    "**Choosing an Optimizer**:\n",
    "\n",
    "- **Adam** is generally a good starting point.\n",
    "- **SGD with Momentum** can be effective but may require more hyperparameter tuning.\n",
    "- **Experimentation** is key; different optimizers may perform better on different datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall Loss function\n",
    "\n",
    "### **5.2 Loss Function**\n",
    "\n",
    "- **`loss='categorical_crossentropy'`**:\n",
    "  - Used for multiclass classification when labels are one-hot encoded.\n",
    "  - Measures the difference between two probability distributions (true labels and predicted probabilities).\n",
    "\n",
    "- **Alternative Loss Functions**:\n",
    "\n",
    "1. **`sparse_categorical_crossentropy`**:\n",
    "   - Used when labels are integers instead of one-hot encoded vectors.\n",
    "   - Example:\n",
    "     - If labels are `[0, 1, 2]` instead of `[[1,0,0], [0,1,0], [0,0,1]]`.\n",
    "\n",
    "2. **`binary_crossentropy`**:\n",
    "   - Used for binary classification tasks.\n",
    "   - Labels are either 0 or 1.\n",
    "\n",
    "**Choosing a Loss Function**:\n",
    "\n",
    "- For **multiclass classification** with one-hot labels, use **`categorical_crossentropy`**.\n",
    "- For **multiclass classification** with integer labels, use **`sparse_categorical_crossentropy`**.\n",
    "- For **binary classification**, use **`binary_crossentropy`**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **6. Training the Model**\n",
    "\n",
    "```python\n",
    "model.fit(train_images, train_labels, epochs=5, batch_size=128)\n",
    "```\n",
    "\n",
    "### **6.1 Epochs**\n",
    "\n",
    "- **`epochs=5`**:\n",
    "  - Number of times the entire training dataset is passed through the model.\n",
    "  - More epochs can lead to better learning but may cause overfitting if too many.\n",
    "\n",
    "\n",
    "### **6.2 Batch Size**\n",
    "\n",
    "- **`batch_size=128`**:\n",
    "  - Number of samples processed before the model is updated.\n",
    "  - **Why 128?**\n",
    "    - It's a commonly used batch size.\n",
    "    - Powers of 2 (e.g., 32, 64, 128, 256) are often chosen for computational efficiency on GPUs.\n",
    "  - **Impact of Batch Size**:\n",
    "    - **Smaller Batch Sizes**:\n",
    "      - More frequent updates.\n",
    "      - Can lead to noisier updates but may generalize better.\n",
    "    - **Larger Batch Sizes**:\n",
    "      - Faster computation per epoch (due to parallelism).\n",
    "      - May require fewer epochs.\n",
    "    - **Trade-off**:\n",
    "      - Small batches can improve generalization but may take longer to train.\n",
    "      - Large batches can speed up training but may require tuning learning rates.\n",
    "\n",
    "**Choosing a Batch Size**:\n",
    "\n",
    "- Experiment with different batch sizes.\n",
    "- Consider computational resources (memory constraints).\n",
    "- Common practice is to start with 32, 64, or 128.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Summary of What Happens in 1 Epoch**:\n",
    "You have **1280 samples**, split into **10 batches**.\n",
    "During the forward pass, each hidden layer sees all the samples in the current batch, processes them, and passes the output to the next layer.\n",
    "After processing each batch, the model calculates the loss and updates the weights in the backward pass.\n",
    "Weights are **updated 10 times** in **1 epoch** (once after each batch).\n",
    "After 10 epochs, the model will have seen the entire dataset 10 times, and the weights will have been updated **100 times in total (10 batches per epoch * 10 epochs)**.\n",
    "\n",
    "**Conclusion:**\n",
    "The key takeaway is that the weights in your hidden layers are updated after every batch of **128 samples**. Therefore, in one epoch, the weights are updated **10 times (once after each batch)**, and this happens regardless of how many layers you have in your network. The hidden layers process each batch independently, and backpropagation adjusts the weights after each batch.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.8722 - loss: 0.4585\n",
      "Epoch 2/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9668 - loss: 0.1169\n",
      "Epoch 3/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9797 - loss: 0.0717\n",
      "Epoch 4/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9857 - loss: 0.0491\n",
      "Epoch 5/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9901 - loss: 0.0349\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1f120fcdcd0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(train_images, train_labels, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **7. Evaluating the Model**\n",
    "\n",
    "```python\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(f'Test accuracy: {test_acc}')\n",
    "```\n",
    "\n",
    "- **`model.evaluate`**:\n",
    "  - Computes the loss and metrics on the test data.\n",
    "- **`test_acc`**:\n",
    "  - The accuracy of the model on the test dataset.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9753 - loss: 0.0802\n",
      "Test accuracy: 0.9789000153541565\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(f'Test accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **8. Alternative Network Variants**\n",
    "\n",
    "To illustrate different variants for the same data, let's explore other network architectures and configurations.\n",
    "\n",
    "### **8.1 Convolutional Neural Network (CNN)**\n",
    "\n",
    "**Why Use CNNs?**\n",
    "\n",
    "- **Spatial Hierarchy**: Preserve spatial relationships in image data.\n",
    "- **Feature Extraction**: Automatically learn features like edges, textures.\n",
    "- **Better Performance**: Often achieve higher accuracy on image data compared to fully connected networks.\n",
    "\n",
    "**CNN Model for MNIST**:\n",
    "\n",
    "```python\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "\n",
    "# Reload data to get original shapes\n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()\n",
    "\n",
    "# Expand dimensions to include channel information\n",
    "train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
    "\n",
    "# One-hot encode labels\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels)\n",
    "\n",
    "# Build CNN model\n",
    "cnn_model = models.Sequential()\n",
    "cnn_model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "cnn_model.add(layers.MaxPooling2D((2, 2)))\n",
    "cnn_model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "cnn_model.add(layers.MaxPooling2D((2, 2)))\n",
    "cnn_model.add(layers.Flatten())\n",
    "cnn_model.add(layers.Dense(64, activation='relu'))\n",
    "cnn_model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "cnn_model.fit(train_images, train_labels, epochs=5, batch_size=64)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = cnn_model.evaluate(test_images, test_labels)\n",
    "print(f'Test accuracy: {test_acc}')\n",
    "```\n",
    "\n",
    "**Explanation**:\n",
    "\n",
    "- **`Conv2D` Layers**:\n",
    "  - Extract features from images using convolutional filters.\n",
    "  - Parameters:\n",
    "    - **Filters**: Number of output filters in the convolution.\n",
    "    - **Kernel Size**: Size of the convolution window.\n",
    "\n",
    "- **`MaxPooling2D` Layers**:\n",
    "  - Reduce spatial dimensions (height and width) to reduce computational load and control overfitting.\n",
    "\n",
    "- **`Flatten` Layer**:\n",
    "  - Flattens the 2D outputs to 1D for the fully connected layers.\n",
    "\n",
    "- **Batch Size**: Reduced to 64 as CNNs are more computationally intensive.\n",
    "\n",
    "### **8.2 Using Different Activation Functions**\n",
    "\n",
    "#### **Replace ReLU with Leaky ReLU**\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(512, input_shape=(28 * 28,)))\n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "```\n",
    "\n",
    "- **LeakyReLU**:\n",
    "  - Allows a small gradient when the unit is not active.\n",
    "  - Can prevent the dying ReLU problem.\n",
    "\n",
    "### **8.3 Using Different Optimizers**\n",
    "\n",
    "#### **Using SGD with Momentum**\n",
    "\n",
    "```python\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "- **SGD with Momentum**:\n",
    "  - Momentum helps accelerate SGD in the relevant direction and dampens oscillations.\n",
    "  - **Learning Rate**: Needs to be specified manually.\n",
    "\n",
    "### **8.4 Using Different Loss Functions**\n",
    "\n",
    "#### **Using Sparse Categorical Crossentropy**\n",
    "\n",
    "- If labels are not one-hot encoded:\n",
    "\n",
    "```python\n",
    "# Do not one-hot encode labels\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "train_images = train_images.reshape((60000, 28 * 28)).astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28 * 28)).astype('float32') / 255\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "- **`sparse_categorical_crossentropy`**:\n",
    "  - Expects integer labels instead of one-hot encoded vectors.\n",
    "  - Can simplify preprocessing steps.\n",
    "\n",
    "### **8.5 Adjusting Batch Size**\n",
    "\n",
    "#### **Smaller Batch Size**\n",
    "\n",
    "```python\n",
    "model.fit(train_images, train_labels, epochs=5, batch_size=32)\n",
    "```\n",
    "\n",
    "- **Advantages**:\n",
    "  - Potentially better generalization.\n",
    "  - More updates per epoch.\n",
    "\n",
    "#### **Larger Batch Size**\n",
    "\n",
    "```python\n",
    "model.fit(train_images, train_labels, epochs=5, batch_size=256)\n",
    "```\n",
    "\n",
    "- **Advantages**:\n",
    "  - Faster computation per epoch due to parallelism.\n",
    "  - Requires less frequent updates.\n",
    "\n",
    "**Note**: When changing batch sizes, consider adjusting the learning rate accordingly.\n",
    "\n",
    "### **8.6 Adding Dropout Layers to Prevent Overfitting**\n",
    "\n",
    "```python\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "```\n",
    "\n",
    "- **`layers.Dropout(0.5)`**:\n",
    "  - Randomly sets 50% of the input units to 0 during training.\n",
    "  - Helps prevent overfitting by reducing reliance on specific neurons.\n",
    "\n",
    "### **8.7 Deeper Network**\n",
    "\n",
    "```python\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(256, activation='relu', input_shape=(28 * 28,)))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "```\n",
    "\n",
    "- **Advantages**:\n",
    "  - Deeper networks can capture more complex patterns.\n",
    "- **Considerations**:\n",
    "  - May require more data to prevent overfitting.\n",
    "  - Training time increases.\n",
    "\n",
    "### **8.8 Using Batch Normalization**\n",
    "\n",
    "```python\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "```\n",
    "\n",
    "- **Batch Normalization**:\n",
    "  - Normalizes the outputs of the previous layer.\n",
    "  - Can speed up training and improve performance.\n",
    "\n",
    "---\n",
    "\n",
    "## **9. Conclusion**\n",
    "\n",
    "- **Flexibility**: TensorFlow and Keras provide flexibility to experiment with different architectures, activation functions, loss functions, optimizers, and batch sizes.\n",
    "- **Experimentation**: The best configuration often depends on the specific dataset and problem. It's crucial to experiment and validate different approaches.\n",
    "- **Best Practices**:\n",
    "  - Start with a simple model and gradually increase complexity.\n",
    "  - Monitor training and validation metrics to detect overfitting.\n",
    "  - Use techniques like dropout and batch normalization to improve generalization.\n",
    "  - Adjust hyperparameters (learning rate, batch size, epochs) based on performance.\n",
    "\n",
    "For better understanding feel free to modify layers options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\didie\\anaconda3\\envs\\pythonAlgo\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 24ms/step - accuracy: 0.8697 - loss: 0.4188\n",
      "Epoch 2/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 22ms/step - accuracy: 0.9832 - loss: 0.0548\n",
      "Epoch 3/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 24ms/step - accuracy: 0.9889 - loss: 0.0368\n",
      "Epoch 4/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 24ms/step - accuracy: 0.9913 - loss: 0.0274\n",
      "Epoch 5/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 25ms/step - accuracy: 0.9936 - loss: 0.0196\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9862 - loss: 0.0402\n",
      "Test accuracy: 0.989300012588501\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import datasets, layers, models\n",
    "\n",
    "# Reload data to get original shapes\n",
    "(train_images, train_labels), (test_images,\n",
    "                               test_labels) = datasets.mnist.load_data()\n",
    "\n",
    "# Expand dimensions to include channel information\n",
    "train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
    "\n",
    "# One-hot encode labels\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels)\n",
    "\n",
    "# Build CNN model\n",
    "cnn_model = models.Sequential()\n",
    "cnn_model.add(layers.Conv2D(\n",
    "    32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "cnn_model.add(layers.MaxPooling2D((2, 2)))\n",
    "cnn_model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "cnn_model.add(layers.MaxPooling2D((2, 2)))\n",
    "cnn_model.add(layers.Flatten())\n",
    "cnn_model.add(layers.Dense(64, activation='relu'))\n",
    "cnn_model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "cnn_model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "cnn_model.fit(train_images, train_labels, epochs=5, batch_size=64)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = cnn_model.evaluate(test_images, test_labels)\n",
    "print(f'Test accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "For **Day 3**, we’ll move into **convolutional neural networks (CNNs)**, which are especially useful for image data like MNIST but on a deeper level. We'll also explore **pooling**, **convolutions**, and how to extract spatial features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSprop and Momentum in Deep Learning\n",
    "\n",
    "Both **RMSprop** and **Momentum** are optimization techniques used to improve the convergence speed and performance of gradient descent in deep learning. They address some of the challenges faced by standard gradient descent, such as oscillations and the vanishing/exploding gradient problem. Here's an explanation of each:\n",
    "\n",
    "---\n",
    "\n",
    "### **Momentum**\n",
    "\n",
    "**Momentum** is a method that helps gradient descent accelerate in the right direction by smoothing the oscillations caused by gradients. It does this by incorporating a fraction of the previous update to the current update, much like how a ball rolling down a hill gains momentum as it continues to move forward.\n",
    "\n",
    "#### How it works:\n",
    "In traditional gradient descent, the update rule for the weights is:\n",
    "\n",
    "$\n",
    "w \\gets w - \\eta \\cdot \\nabla L(w)\n",
    "$\n",
    "\n",
    "Where:\n",
    "- $ \\eta $ is the learning rate.\n",
    "- $ \\nabla L(w) $ is the gradient of the loss function with respect to the weights.\n",
    "\n",
    "With **momentum**, we introduce a velocity term, $ v $, which accumulates the gradients over time:\n",
    "\n",
    "$\n",
    "v \\gets \\beta v - \\eta \\cdot \\nabla L(w)\n",
    "$\n",
    "$\n",
    "w \\gets w + v\n",
    "$\n",
    "\n",
    "Where:\n",
    "- $ v $ is the velocity (momentum) of the weights.\n",
    "- $ \\beta $ is the momentum coefficient (usually between 0.9 and 0.99), controlling how much of the previous velocity is retained.\n",
    "\n",
    "#### Intuition:\n",
    "- Momentum helps the optimizer move through **flat regions** (where gradients are small) more quickly by maintaining some velocity.\n",
    "- It also reduces **oscillations** when descending into narrow valleys of the loss surface because it dampens the influence of noisy gradient updates by smoothing them out.\n",
    "\n",
    "#### Benefits:\n",
    "- Faster convergence, especially in scenarios with **high curvature** or **flat regions**.\n",
    "- Reduces the oscillations caused by steep gradients in some directions, leading to more stable updates.\n",
    "\n",
    "---\n",
    "\n",
    "### **RMSprop (Root Mean Square Propagation)**\n",
    "\n",
    "**RMSprop** is an adaptive learning rate optimization algorithm that adjusts the learning rate for each parameter individually, based on the recent history of gradients. This helps the optimizer deal with the vanishing/exploding gradient problem and maintain a steady learning process.\n",
    "\n",
    "#### How it works:\n",
    "RMSprop maintains a moving average of the squared gradients for each parameter, which helps scale the learning rate appropriately.\n",
    "\n",
    "The update rule for RMSprop is:\n",
    "\n",
    "1. Update the exponentially decaying average of the squared gradients:\n",
    "   \n",
    "   $\n",
    "   E[g^2]_t \\gets \\beta E[g^2]_{t-1} + (1 - \\beta) g_t^2\n",
    "   $\n",
    "   \n",
    "   Where:\n",
    "   - $ g_t $ is the gradient at time step $ t $.\n",
    "   - $ E[g^2]_t $ is the moving average of the squared gradients.\n",
    "   - $ \\beta $ is the decay rate (commonly set to 0.9).\n",
    "\n",
    "2. Update the weights:\n",
    "\n",
    "   $\n",
    "   w \\gets w - \\frac{\\eta}{\\sqrt{E[g^2]_t + \\epsilon}} g_t\n",
    "   $\n",
    "\n",
    "   Where:\n",
    "   - $ \\eta $ is the global learning rate.\n",
    "   - $ \\epsilon $ is a small constant added to prevent division by zero (usually $ 10^{-8} $).\n",
    "\n",
    "#### Intuition:\n",
    "- RMSprop adapts the learning rate based on how large or small the gradients have been for each parameter, meaning parameters with large gradients get smaller updates, and parameters with small gradients get larger updates.\n",
    "- This adjustment helps prevent large, noisy gradients from causing overly large updates, while ensuring small gradients still make meaningful progress.\n",
    "\n",
    "#### Benefits:\n",
    "- **Effective for non-stationary loss surfaces**: RMSprop performs well when the gradient magnitudes change frequently (non-stationary problems).\n",
    "- **Prevents vanishing/exploding gradients**: By adjusting the learning rate based on the squared gradients, RMSprop helps stabilize the training process, especially in deep networks where vanishing/exploding gradients can be problematic.\n",
    "\n",
    "---\n",
    "\n",
    "### Comparison: **Momentum vs. RMSprop**\n",
    "- **Momentum** accelerates gradient descent by building velocity and smoothing oscillations. It works well when the optimizer faces challenges with high curvature, small gradients, or oscillations.\n",
    "  \n",
    "- **RMSprop** adapts the learning rate for each parameter individually based on the recent gradient history. It excels at solving problems where the gradients are highly variable or non-stationary, and it helps stabilize the training process.\n",
    "\n",
    "### **Combining Momentum and RMSprop**\n",
    "Interestingly, both techniques can be combined. The **Adam optimizer** (Adaptive Moment Estimation) incorporates both **momentum** and **RMSprop** principles by using momentum for first-order gradients (like in momentum) and RMSprop-like behavior for second-order gradients (adaptively scaling the learning rate).\n",
    "\n",
    "In summary:\n",
    "- **Momentum** adds velocity to the gradient updates, helping traverse flat regions and dampen oscillations.\n",
    "- **RMSprop** adjusts the learning rate for each parameter individually based on the recent history of gradients, preventing large updates in some directions and making training more stable.\n",
    "\n",
    "These optimizers improve upon vanilla gradient descent, making them effective choices in deep learning models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonAlgo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
