{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 1: Understanding the Basics of Neural Networks\n",
    "\n",
    "#### 1. **Neuron Formula: The Building Block**\n",
    "A neuron in a neural network is inspired by the human brain. It takes inputs, processes them, and produces an output. Let’s break down the formula of a **perceptron** (the simplest form of a neuron):\n",
    "\n",
    "- **Inputs**: $ x_1, x_2, ..., x_n $ (features)\n",
    "- **Weights**: $ w_1, w_2, ..., w_n $ (how important each input is)\n",
    "- **Bias**: $ b $ (shifts the activation function, so the model is more flexible)\n",
    "- **Output**: The perceptron’s output is a weighted sum of the inputs, passed through an activation function.\n",
    "\n",
    "$$ z = w_1x_1 + w_2x_2 + ... + w_nx_n + b $$\n",
    " \n",
    "This $$ z $$ is then passed through an **activation function** to give the final output, typically something like:\n",
    "\n",
    "$$ y = f(z) $$\n",
    "\n",
    "Here, $ f(z) $ could be a **sigmoid function**, a **ReLU** (Rectified Linear Unit), or another activation function.\n",
    "\n",
    "#### 2. **Forward Propagation: Passing Inputs Through the Network**\n",
    "In **forward propagation**, the data passes through all the layers of the network:\n",
    "\n",
    "- The inputs are passed to each neuron.\n",
    "- The neuron computes the weighted sum of inputs plus bias, and the result is transformed using the activation function.\n",
    "- This continues through all the layers, from the input to the output layer, where the final prediction is made.\n",
    "\n",
    "**Goal**: Make a prediction based on the input.\n",
    "\n",
    "#### 3. **Backpropagation: Learning From Mistakes**\n",
    "Neural networks learn through **backpropagation**. After predicting an output, we compare it with the true value using a **loss function** (e.g., Mean Squared Error for regression or Cross Entropy for classification). \n",
    "\n",
    "To improve the model, we need to adjust the weights and biases to reduce the error:\n",
    "\n",
    "1. **Calculate the error**: Compare the predicted output to the true output using a loss function.\n",
    "2. **Compute gradients**: Using calculus, calculate how much each weight contributed to the error (this is done through the chain rule).\n",
    "3. **Update weights**: Adjust the weights and biases slightly to reduce the error. This is done using **gradient descent**.\n",
    "\n",
    "$$ w_i \\gets w_i - \\eta \\frac{\\partial L}{\\partial w_i} $$\n",
    "\n",
    "Where $$ \\eta $$ is the learning rate and $$ \\frac{\\partial L}{\\partial w_i} $$ is the gradient of the loss with respect to the weight $$ w_i $$.\n",
    "\n",
    "**Goal**: Minimize the loss by adjusting weights and biases to improve the accuracy of the model.\n",
    "\n",
    "#### 4. **Activation Functions: The Decision Makers**\n",
    "- **Sigmoid**: Squashes the output between 0 and 1, useful for probabilities.\n",
    "  \n",
    "  $$ f(z) = \\frac{1}{1 + e^{-z}} $$\n",
    "  \n",
    "- **ReLU (Rectified Linear Unit)**: Output is 0 if $ z \\leq 0 $, and $ z $ if $ z > 0 $. Great for deeper networks because it avoids the vanishing gradient problem.\n",
    "\n",
    "  $$ f(z) = \\max(0, z) $$\n",
    "\n",
    "- **Softmax**: Used in the output layer for multi-class classification. It converts raw scores into probabilities that sum to 1.\n",
    "\n",
    "#### 5. **Programming from Scratch**\n",
    "\n",
    "We’ll start by coding a simple perceptron in Python:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s a breakdown of the perceptron code, explaining each part:\n",
    "\n",
    "### 1. **Imports**\n",
    "```python\n",
    "import numpy as np\n",
    "```\n",
    "- **`numpy`** is imported as `np` to handle arrays and mathematical operations efficiently, such as dot products.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Activation Function**\n",
    "```python\n",
    "def activation_function(x):\n",
    "    return 1 if x >= 0 else 0\n",
    "```\n",
    "- The **activation function** determines the perceptron’s output. In this case, it's a simple **step function**, which returns `1` if the input `x` is greater than or equal to `0` and `0` otherwise.\n",
    "- The perceptron fires (returns `1`) if the input signal is strong enough (non-negative), and doesn't fire (returns `0`) if the signal is weak (negative).\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Perceptron Class**\n",
    "#### 3.1. **Initialization**\n",
    "```python\n",
    "class Perceptron:\n",
    "    def __init__(self, input_size, learning_rate=0.1):\n",
    "        self.weights = np.zeros(input_size + 1)  # +1 for the bias term\n",
    "        self.learning_rate = learning_rate\n",
    "```\n",
    "- The **`Perceptron` class** defines the model.\n",
    "- **`input_size`** specifies how many input features there are. The perceptron is built for binary classification, receiving a number of inputs.\n",
    "- The **weights** are initialized to zeros using `np.zeros`. There’s one weight for each input, plus one for the **bias term** (which controls the threshold for the decision boundary).\n",
    "- **`learning_rate`** defines the step size for updating the weights during training.\n",
    "\n",
    "#### 3.2. **Predict Function**\n",
    "```python\n",
    "def predict(self, inputs):\n",
    "    summation = np.dot(inputs, self.weights[1:]) + self.weights[0]\n",
    "    return activation_function(summation)\n",
    "```\n",
    "- **`predict`** computes the weighted sum of the inputs:\n",
    "    - `np.dot(inputs, self.weights[1:])` calculates the dot product between the input values and the weights (excluding the bias).\n",
    "    - **`self.weights[0]`** is the bias term added to the summation.\n",
    "- The **activation function** then decides the output based on the summation (whether it's 0 or 1).\n",
    "  \n",
    "#### 3.3. **Training Function**\n",
    "```python\n",
    "def train(self, training_inputs, labels, epochs=10):\n",
    "    for _ in range(epochs):  # Loop over the dataset multiple times\n",
    "        for inputs, label in zip(training_inputs, labels):\n",
    "            prediction = self.predict(inputs)\n",
    "            self.weights[1:] += self.learning_rate * (label - prediction) * inputs\n",
    "            self.weights[0] += self.learning_rate * (label - prediction)\n",
    "```\n",
    "- **`train`** method performs the learning process:\n",
    "  - **`training_inputs`**: The data that will be used to train the perceptron (e.g., `[[0,0], [0,1], [1,0], [1,1]]` for an AND gate).\n",
    "  - **`labels`**: The true output for each input (e.g., `[0, 0, 0, 1]` for an AND gate).\n",
    "  - **`epochs`**: Number of times the model goes through the entire dataset. Training over multiple epochs ensures that the perceptron updates its weights until it converges on correct predictions.\n",
    "\n",
    "- For each input-label pair:\n",
    "  1. **Prediction**: The current input is passed to `self.predict(inputs)` to get the perceptron's output.\n",
    "  2. **Weight update**: The weights are updated based on the error `(label - prediction)`. If the prediction is wrong, the weights are adjusted to reduce future errors:\n",
    "     - **`self.weights[1:]`**: The weights associated with the inputs are updated.\n",
    "     - **`self.weights[0]`**: The bias weight is updated.\n",
    "     - The amount of update is determined by the learning rate and the error.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Training Data**\n",
    "```python\n",
    "training_inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "labels = np.array([0, 0, 0, 1])  # AND gate\n",
    "```\n",
    "- These are the training inputs and corresponding labels for the **AND gate** problem.\n",
    "  - Inputs are all possible pairs of 0s and 1s: `[0, 0]`, `[0, 1]`, `[1, 0]`, `[1, 1]`.\n",
    "  - Labels are the expected outputs: \n",
    "    - `0 AND 0` = 0\n",
    "    - `0 AND 1` = 0\n",
    "    - `1 AND 0` = 0\n",
    "    - `1 AND 1` = 1\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Creating and Training the Perceptron**\n",
    "```python\n",
    "perceptron = Perceptron(input_size=2)\n",
    "perceptron.train(training_inputs, labels, epochs=10)\n",
    "```\n",
    "- A perceptron is created with **2 input features** (`input_size=2`).\n",
    "- The perceptron is trained on the **AND gate** dataset for **10 epochs**.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Testing the Perceptron**\n",
    "```python\n",
    "print(perceptron.predict(np.array([0, 0])))  # Expected output: 0\n",
    "print(perceptron.predict(np.array([1, 1])))  # Expected output: 1\n",
    "print(perceptron.predict(np.array([0, 1])))  # Expected output: 0\n",
    "print(perceptron.predict(np.array([1, 0])))  # Expected output: 0\n",
    "```\n",
    "- After training, the perceptron is tested with inputs to verify that it has learned the AND gate logic:\n",
    "  - `[0, 0]` should return `0`.\n",
    "  - `[1, 1]` should return `1`.\n",
    "  - `[0, 1]` should return `0`.\n",
    "  - `[1, 0]` should return `0`.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Concepts:\n",
    "- **Perceptron**: A binary classifier that can learn simple decision boundaries.\n",
    "- **Weights and Bias**: Parameters that determine how input values are transformed into an output.\n",
    "- **Learning**: Adjusting weights through training to minimize errors.\n",
    "- **Activation Function**: Decides the output based on the weighted sum of inputs.\n",
    "- **Epochs**: Number of times the model is trained over the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### 5. **Programming from Scratch**\n",
    "\n",
    "We’ll start by coding a simple perceptron in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Activation function (Step function for simplicity)\n",
    "def activation_function(x):\n",
    "    return 1 if x >= 0 else 0\n",
    "\n",
    "\n",
    "# Perceptron class\n",
    "class Perceptron:\n",
    "    def __init__(self, input_size, learning_rate=0.1):\n",
    "        self.weights = np.zeros(input_size + 1)  # +1 for the bias term\n",
    "        self.learning_rate = learning_rate\n",
    "        # self.epochs = 5\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        summation = np.dot(inputs, self.weights[1:]) + self.weights[0]\n",
    "        return activation_function(summation)\n",
    "\n",
    "    def train(self, training_inputs, labels, epochs=10):\n",
    "        for _ in range(epochs):\n",
    "            for inputs, label in zip(training_inputs, labels):\n",
    "                prediction = self.predict(inputs)\n",
    "                self.weights[1:] += self.learning_rate * (label - prediction) * inputs\n",
    "                self.weights[0] += self.learning_rate * (label - prediction)\n",
    "\n",
    "\n",
    "# Sample training data\n",
    "training_inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "labels = np.array([0, 0, 0, 1])  # AND gate\n",
    "\n",
    "# Create perceptron and train\n",
    "perceptron = Perceptron(input_size=2)\n",
    "perceptron.train(training_inputs, labels, 5)\n",
    "\n",
    "# Test the perceptron\n",
    "# print(perceptron.predict(np.array([0, 0])))  # Expected output: 0\n",
    "# print(perceptron.predict(np.array([0, 1])))  # Expected output: 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(perceptron.predict(np.array([0, 0])))  # Expected output: 0\n",
    "print(perceptron.predict(np.array([1, 1])))  # Expected output: 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Activation function (Step function for simplicity)\n",
    "\n",
    "\n",
    "def activation_function(x):\n",
    "    return 1 if x >= 0 else 0\n",
    "\n",
    "\n",
    "# Perceptron class\n",
    "\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, input_size, learning_rate=0.1):\n",
    "        self.weights = np.zeros(input_size + 1)  # +1 for the bias term\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        summation = np.dot(inputs, self.weights[1:]) + self.weights[0]\n",
    "        return activation_function(summation)\n",
    "\n",
    "    def train(self, training_inputs, labels, epochs=10):\n",
    "        for _ in range(epochs):  # Loop over the dataset multiple times\n",
    "            for inputs, label in zip(training_inputs, labels):\n",
    "                prediction = self.predict(inputs)\n",
    "                self.weights[1:] += self.learning_rate * (label - prediction) * inputs\n",
    "                self.weights[0] += self.learning_rate * (label - prediction)\n",
    "\n",
    "\n",
    "# Sample training data\n",
    "training_inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "labels = np.array([0, 0, 0, 1])  # AND gate\n",
    "\n",
    "# Create perceptron and train\n",
    "perceptron = Perceptron(input_size=2)\n",
    "perceptron.train(training_inputs, labels, epochs=10)\n",
    "\n",
    "# Test the perceptron\n",
    "print(perceptron.predict(np.array([0, 0])))  # Expected output: 0\n",
    "print(perceptron.predict(np.array([1, 1])))  # Expected output: 1\n",
    "print(perceptron.predict(np.array([0, 1])))  # Expected output: 0\n",
    "print(perceptron.predict(np.array([1, 0])))  # Expected output: 0\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
