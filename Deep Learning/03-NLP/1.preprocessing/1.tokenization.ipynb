{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "To understand a sentence, us as humans, read each word. We analyze the meaning of it and then we connect words together. It's the same for a machine. So the first step to almost any NLP task will be to slice sentences up in words.\n",
    "\n",
    "![tokens](https://www.kdnuggets.com/wp-content/uploads/text-tokens-tokenization-manning.jpg)\n",
    "\n",
    "It seems simple said like that, but you also have to slice punctuation, composed words (but not all of them),...\n",
    "\n",
    "It's a time consuming task, that's why people invented a \"Tokenization\" function.\n",
    "\n",
    "Let's have a look at how [Spacy](https://spacy.io/) handles that.\n",
    "\n",
    "## Installation\n",
    "\n",
    "You will need to install Spacy, to do that I let you search on [their website](https://spacy.io/).\n",
    "You will also need to download their `en_core_web_sm`. To do that you can type:\n",
    "\n",
    "```shell\n",
    "python -m spacy download en_core_web_sm\n",
    "```\n",
    "\n",
    "## Tokenize the text\n",
    "\n",
    "Now that you installed Spacy, let's take a look at their basic example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the tokens in doc\n",
    "# Load English tokenizer, tagger, parser and NER\n",
    "EN_nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Load FRENCH tokenizer, tagger, parser and NER\n",
    "FR_nlp = spacy.load(\"fr_core_news_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_text = text_fr = (\n",
    "    \"En 2015, Clara Dupont a décidé de se lancer dans la recherche sur l'intelligence artificielle,\"\n",
    "    \"un domaine alors dominé par quelques grandes entreprises technologiques.\"\n",
    "    \"« À l’époque, très peu de chercheurs en France s’intéressaient aux réseaux neuronaux profonds », \"\n",
    "    \"explique-t-elle. « Beaucoup pensaient que ces modèles étaient trop complexes pour être utiles en pratique. »\"\n",
    "    \"Aujourd’hui, ses travaux sont reconnus internationalement et contribuent à des avancées majeures dans le domaine.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_text = (\n",
    "    \"When Sebastian Thrun started working on self-driving cars at \"\n",
    "    \"Google in 2007, few people outside of the company took him \"\n",
    "    \"seriously. “I can tell you very senior CEOs of major American \"\n",
    "    \"car companies would shake my hand and turn away because I wasn’t \"\n",
    "    \"worth talking to,” said Thrun, in an interview with Recode earlier \"\n",
    "    \"this week.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using langdetect we are able to guess the text language\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fr\n",
      "en\n"
     ]
    }
   ],
   "source": [
    "print(detect(fr_text))\n",
    "print(detect(en_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text_autodetect(text: str = \"\"):\n",
    "    print(\"EN detected\" if detect(text) == \"en\" else \"FR detected\")\n",
    "    return EN_nlp(text) if detect(text) == \"en\" else FR_nlp(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR detected\n"
     ]
    }
   ],
   "source": [
    "doc = load_text_autodetect(fr_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN detected\n"
     ]
    }
   ],
   "source": [
    "doc = load_text_autodetect(en_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parsing text in token (tokeninsation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun phrases : ['Sebastian Thrun', 'self-driving cars', 'Google', 'few people', 'the company', 'him', 'I', 'you', 'very senior CEOs', 'major American car companies', 'my hand', 'I', 'Thrun', 'an interview', 'Recode']\n",
      "Verbs: ['start', 'work', 'drive', 'take', 'tell', 'shake', 'turn', 'talk', 'say']\n"
     ]
    }
   ],
   "source": [
    "print(\"Noun phrases :\", [chunk.text for chunk in doc.noun_chunks])\n",
    "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## describing various entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sebastian Thrun PERSON\n",
      "Google ORG\n",
      "2007 DATE\n",
      "American NORP\n",
      "Thrun GPE\n",
      "Recode ORG\n",
      "earlier this week DATE\n"
     ]
    }
   ],
   "source": [
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect, our text is tokenized, now we can see a lot of interesting features. But first of all, let's see what our tokens look like:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When\n",
      "Sebastian\n",
      "Thrun\n",
      "started\n",
      "working\n",
      "on\n",
      "self\n",
      "-\n",
      "driving\n",
      "cars\n",
      "at\n",
      "Google\n",
      "in\n",
      "2007\n",
      ",\n",
      "few\n",
      "people\n",
      "outside\n",
      "of\n",
      "the\n",
      "company\n",
      "took\n",
      "him\n",
      "seriously\n",
      ".\n",
      "“\n",
      "I\n",
      "can\n",
      "tell\n",
      "you\n",
      "very\n",
      "senior\n",
      "CEOs\n",
      "of\n",
      "major\n",
      "American\n",
      "car\n",
      "companies\n",
      "would\n",
      "shake\n",
      "my\n",
      "hand\n",
      "and\n",
      "turn\n",
      "away\n",
      "because\n",
      "I\n",
      "was\n",
      "n’t\n",
      "worth\n",
      "talking\n",
      "to\n",
      ",\n",
      "”\n",
      "said\n",
      "Thrun\n",
      ",\n",
      "in\n",
      "an\n",
      "interview\n",
      "with\n",
      "Recode\n",
      "earlier\n",
      "this\n",
      "week\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should look something like this:\n",
    "\n",
    "```\n",
    "When\n",
    "Sebastian\n",
    "Thrun\n",
    "started\n",
    "working\n",
    "on\n",
    "self\n",
    "-\n",
    "driving\n",
    "cars\n",
    "at\n",
    "Google\n",
    "in\n",
    "2007\n",
    ",\n",
    "few\n",
    "people\n",
    "outside\n",
    "of\n",
    "the\n",
    "company\n",
    "took\n",
    "him\n",
    "seriously\n",
    ".\n",
    "“\n",
    "I\n",
    "can\n",
    "tell\n",
    "you\n",
    "very\n",
    "senior\n",
    "CEOs\n",
    "of\n",
    "major\n",
    "American\n",
    "car\n",
    "companies\n",
    "would\n",
    "shake\n",
    "my\n",
    "hand\n",
    "and\n",
    "turn\n",
    "away\n",
    "because\n",
    "I\n",
    "was\n",
    "n’t\n",
    "worth\n",
    "talking\n",
    "to\n",
    ",\n",
    "”\n",
    "said\n",
    "Thrun\n",
    ",\n",
    "in\n",
    "an\n",
    "interview\n",
    "with\n",
    "Recode\n",
    "earlier\n",
    "this\n",
    "week\n",
    ".\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the punctuation and `-` have been separated from the word they were appended to.\n",
    "\n",
    "Spacy also applies a lot of other preprocessing steps that we will see later.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
